##############################################################################
#
#   Snakemake workflow to run demon (deme-based oncology model)
#   
#   AUTHOR: Maciej_Bak
#   CONTACT: wsciekly.maciek@gmail.com
#   CREATED: 15-02-2022
#   LICENSE: Apache_2.0
#
##############################################################################


# import required modules/packages/functions
import glob
import itertools
from math import prod
import os
import traceback
from jinja2 import Template


# internal path to the demon configfile template
demon_config_template = os.path.join(
        config["workflow_repo_path"],
        "resources",
        "template-config.dat"
)


def CreateConfigFilesCrossProduct():
    """Render demon config template with the parameters from YAML;
    implement Cartesian product of all possible parameters' options.

    Args:
        objects for this workflow; required for snakemake
    """

    demon_params = [k for k in list(config.keys()) if k.startswith("demon_")]
    variable_params = [k for k in demon_params if type(config[k]) is list]
    list_of_lists = [config[key] for key in variable_params]
    cartesian_products = [_ for _ in itertools.product(*list_of_lists)]
    N = prod([len(config[k]) for k in variable_params])
    hexnames = [hex(_).split("x")[-1].upper().rjust(4, "0") for _ in range(N)]

    # dict of configfiles for demon:
    # run ID -> demon config template rendered string
    configfiles = {}

    for dirname, c_prod in zip(hexnames, cartesian_products):

        # render the config dict first
        config_copy = dict(config) # deep copy of a Python dict
        for _ in range(len(variable_params)):
            config_copy[variable_params[_]] = c_prod[_]

        # then render the DAT template
        with open(demon_config_template) as f: template = Template(f.read())
        configfiles[dirname] = template.render(config_copy)

    return configfiles


# prepare the encoded configfiles for demon
# (before the pipeline's init)
ENCODED_DEMON_CONFIGFILES = CreateConfigFilesCrossProduct()


##############################################################################


# define always-local rules
localrules: all, prepare_configfiles


# before the workflow starts: create the main outdir
onstart:
    os.makedirs(
        config["workflow_analysis_outdir"],
        exist_ok=True
    )


rule prepare_configfiles:
    """
    Prepare separate simulation config files based on the main workflow config.
    """
    output:
        DAT_demon_configfile = os.path.join(
            "{outdir}",
            "configfiles",
            "{runID}.dat"
        )

    params:
        STR_runID = "{runID}",
        DIR_demon_configfile_dirpath = os.path.join(
            "{outdir}",
            "configfiles"
        )

    threads: 1

    log:
        LOG_local_stdout = os.path.join(
            "{outdir}",
            "logs",
            "prepare_configfiles.{runID}.stdout.log"
        ),
        LOG_local_stderr = os.path.join(
            "{outdir}",
            "logs",
            "prepare_configfiles.{runID}.stderr.log"
        )

    benchmark:
        os.path.join(
            "{outdir}",
            "logs",
            "prepare_configfiles.{runID}.benchmark.log"
        )

    run:
        with open(log.LOG_local_stderr, "w") as errlogfile:
            try:
                os.makedirs(
                    params.DIR_demon_configfile_dirpath,
                    exist_ok=True
                )
                with open(output.DAT_demon_configfile, "w") as f:
                    f.write(ENCODED_DEMON_CONFIGFILES[params.STR_runID])
            except Exception:
                traceback.print_exc(file = errlogfile)
                raise Exception(
                    "Workflow error at rule: prepare_configfiles"
                )


rule run_demon:
    """
    Run demon simulations in parallel for every config separately.
    """
    input:
        BIN_demon = os.path.join(
            config["workflow_repo_path"],
            "resources",
            "demon_model",
            "bin",
            "demon"
        ),
        DAT_demon_configfile = os.path.join(
            "{outdir}",
            "configfiles",
            "{runID}.dat"
        )

    output:
        DIR_simulations_outdir = directory(
            os.path.join(
                "{outdir}",
                "simulations",
                "{runID}"
            )
        )

    params:
        STR_runID = "{runID}",
        LOG_cluster_log = os.path.join(
            "{outdir}",
            "logs",
            "run_demon.{runID}.cluster.log"
        )

    threads: 1

    log:
        LOG_local_stdout = os.path.join(
            "{outdir}",
            "logs",
            "run_demon.{runID}.stdout"
        ),
        LOG_local_stderr = os.path.join(
            "{outdir}",
            "logs",
            "run_demon.{runID}.stderr"
        )

    benchmark:
        os.path.join(
            "{outdir}",
            "logs",
            "run_demon.{runID}.benchmark.log"
        )

    shell:
        # Tiny workaround since demon creates output in the exact same
        # dir where its configfile is located in.
        # (small nap time for possible filesystem delays)
        """
        (mkdir {output.DIR_simulations_outdir} \
        && \
        sleep 5 \
        && \
        cp \
        {input.DAT_demon_configfile} \
        {output.DIR_simulations_outdir}/config.dat \
        && \
        {input.BIN_demon} \
        {output.DIR_simulations_outdir} \
        config.dat \
        rm -f {output.DIR_simulations_outdir}/config.dat) \
        1> {log.LOG_local_stdout} \
        2> {log.LOG_local_stderr}
        """


rule all:
    """
    Target rule gathering final output of the workflow.
    """
    input:
        DIR_simulations_outdir = expand(
            os.path.join(
                "{outdir}",
                "simulations",
                "{runID}"
            ),
            outdir = config["workflow_analysis_outdir"],
            runID = ENCODED_DEMON_CONFIGFILES.keys()
        )
